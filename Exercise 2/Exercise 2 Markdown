\documentclass{article}
\usepackage{enumitem}
\usepackage{fancyhdr}
\usepackage{extramarks}
\usepackage{amsmath}
\usepackage{amsthm}
\usepackage{amsfonts}
\usepackage{tikz}
\usepackage[plain]{algorithm}
\usepackage{algpseudocode}
\usepackage{graphicx}
\usepackage{subfig}

\graphicspath{ {images/} }


\usetikzlibrary{automata,positioning}

%
% Basic Document Settings
%

\topmargin=-0.45in
\evensidemargin=0in
\oddsidemargin=0in
\textwidth=6.5in
\textheight=9.0in
\headsep=0.25in

\linespread{1.1}

\pagestyle{fancy}
%\lhead{\hmwkAuthorName}
\lhead{\hmwkClass\ (\hmwkClassTime): \hmwkTitle}
\rhead{\firstxmark}
\lfoot{\lastxmark}
\cfoot{\thepage}

\renewcommand\headrulewidth{0.4pt}
\renewcommand\footrulewidth{0.4pt}

\setlength\parindent{0pt}

%
% Create Problem Sections
%

\newcommand{\enterProblemHeader}[1]{
    \nobreak\extramarks{}{Problem \arabic{#1} continued on next page\ldots}\nobreak{}
    \nobreak\extramarks{Problem \arabic{#1} (continued)}{Problem \arabic{#1} continued on next page\ldots}\nobreak{}
}

\newcommand{\exitProblemHeader}[1]{
    \nobreak\extramarks{Problem \arabic{#1} (continued)}{Problem \arabic{#1} continued on next page\ldots}\nobreak{}
    \stepcounter{#1}
    \nobreak\extramarks{Problem \arabic{#1}}{}\nobreak{}
}

\setcounter{secnumdepth}{0}
\newcounter{partCounter}
\newcounter{homeworkProblemCounter}
\setcounter{homeworkProblemCounter}{1}
\nobreak\extramarks{Problem \arabic{homeworkProblemCounter}}{}\nobreak{}

%
% Homework Problem Environment
%
% This environment takes an optional argument. When given, it will adjust the
% problem counter. This is useful for when the problems given for your
% assignment aren't sequential. See the last 3 problems of this template for an
% example.
%
\newenvironment{homeworkProblem}[1][-1]{
    \ifnum#1>0
        \setcounter{homeworkProblemCounter}{#1}
    \fi
    \section{Problem \arabic{homeworkProblemCounter}}
    \setcounter{partCounter}{1}
    \enterProblemHeader{homeworkProblemCounter}
}{
    \exitProblemHeader{homeworkProblemCounter}
}

%
% Homework Details
%   - Title
%   - Due date
%   - Class
%   - Section/Time
%   - Instructor
%   - Author
%

\newcommand{\hmwkTitle}{Exercise 2}
% Change Date Here
\newcommand{\hmwkDueDate}{August 18, 2017}
% Change Class Name Here
\newcommand{\hmwkClass}{Predictive Models}
\newcommand{\hmwkClassTime}{MSBA}
\newcommand{\hmwkClassInstructor}{Professor James Scott}
\newcommand{\hmwkAuthorName}{\textbf{Matt Barrett} \and \textbf{Timothy Lai} \and \textbf{Brett Scroggins} \and \textbf{Meyappan Subbaiah}}

%
% Title Page
%

\title{
    \vspace{2in}
    \textmd{\textbf{\hmwkClass:\ \hmwkTitle}}\\
    \normalsize\vspace{0.1in}\small{Due\ on\ \hmwkDueDate\ at 5:00pm}\\
    \vspace{0.1in}\large{\textit{\hmwkClassInstructor}}
    \vspace{3in}
}

\author{\hmwkAuthorName}
\date{}

\renewcommand{\part}[1]{\textbf{\large Part \Alph{partCounter}}\stepcounter{partCounter}\\}

%
% Various Helper Commands
%

% Useful for algorithms
\newcommand{\alg}[1]{\textsc{\bfseries \footnotesize #1}}

% For derivatives
\newcommand{\deriv}[1]{\frac{\mathrm{d}}{\mathrm{d}x} (#1)}

% For partial derivatives
\newcommand{\pderiv}[2]{\frac{\partial}{\partial #1} (#2)}

% Integral dx
\newcommand{\dx}{\mathrm{d}x}

% Alias for the Solution section header
\newcommand{\solution}{\textbf{\large Solution}}

% Probability commands: Expectation, Variance, Covariance, Bias
\newcommand{\E}{\mathrm{E}}
\newcommand{\Var}{\mathrm{Var}}
\newcommand{\Cov}{\mathrm{Cov}}
\newcommand{\Bias}{\mathrm{Bias}}
\setlength{\parskip}{6pt}

\begin{document}

<<echo=FALSE, warning=FALSE>>=
library(knitr)
knit_hooks$set(document = function(x) {sub('\\usepackage[]{color}', '\\usepackage{xcolor}', x, fixed = TRUE)})
@


\maketitle

\pagebreak

\begin{homeworkProblem}
\textbf{Exploratory Analysis: Austin Flight Delays}\par

Create a figure, or set of related figures, that tell an interesting story about flights into and out of Austin. You can annotate the figure and briefly describe it, but strive to make it as stand-alone as possible. It shouldn't need many, many paragraphs to convey its meaning. Rather, the figure should speak for itself as far as possible. For example, you might consider one of the following questions:

\textbf{What is the best time of day to fly to minimize delays?}\par
\textbf{What is the best time of year to fly to minimize delays?}\par
\textbf{How do patterns of flights to different destinations or parts of the country change over the course of the year?}\par
\textbf{What are the bad airports to fly to?}\par
But anything interesting will fly.\par

\textbf{Solution:}

<<eval=FALSE,warning=FALSE>>=
options(stringsAsFactors = FALSE)
austin_flights_df <- read.csv("https://raw.githubusercontent.com/jgscott/STA380/master/data/ABIA.csv")

library(tidyverse)

ggplot(austin_flights_df) + 
  geom_histogram(aes(DepDelay))

### Time Series Calendar HeatMap
avg_delay <- austin_flights_df %>% group_by(Month,DayofMonth) %>% 
  summarise(dep_delay = mean(DepDelay[DepDelay>0],na.rm=T),
            arr_delay = mean(ArrDelay[ArrDelay>0],na.rm=T)) %>% ungroup() %>% 
  mutate(Year = 2008,
         monthweek = ceiling(DayofMonth/7),
         date = as.Date(paste(Year, Month, DayofMonth,sep="-"), "%Y-%m-%d"),
         day = weekdays(date),
         mnth = month.abb[Month],
         mnth = factor(mnth,levels=c("Jan","Feb","Mar",
                                               "Apr","May","Jun",
                                               "Jul","Aug","Sep",
                                               "Oct","Nov","Dec")),
         day = factor(day, levels= rev(c("Sunday", "Monday", 
                                                   "Tuesday", "Wednesday", "Thursday", "Friday", "Saturday"))))

 p1 <- ggplot(avg_delay, aes(monthweek, day, fill = dep_delay)) + 
  geom_tile(colour = "white") + 
  facet_wrap(~mnth,ncol=3) + 
  scale_fill_gradient(low='yellow', high="red",name='Delay (minutes)',
                      breaks = c(10, 20, 30, 40, 50,60)) +
  labs(title = 'Average Flight Departures Delays',
       x = 'Week of Month', 
       y = '') + theme_bw() + theme(aspect.ratio = 1) 

p4 <- ggplot(avg_delay, aes(monthweek, day, fill = arr_delay)) + 
  geom_tile(colour = "white") + 
  facet_wrap(~mnth,ncol=3) + 
  scale_fill_gradient(low='yellow', high="red",name='Delay (minutes)',
                      breaks = c(10, 20, 30, 40, 50,60)) +
  labs(title = 'Average Flight Arrival Delays',
       x = 'Week of Month', 
       y = '') + theme_bw() + theme(aspect.ratio = 1) 


library(maps)
library(ggmap)

# a set of personal choices for the map display
states <- map_data("state")
map.opts <- theme(panel.grid.minor=element_blank(), panel.grid.major=element_blank(), panel.background=element_blank(), 
                  axis.title.x=element_blank(), axis.title.y=element_blank(), 
                  axis.line=element_blank(), axis.ticks=element_blank(), 
                  axis.text.y = element_text(colour="#FFFFFF"), 
                  axis.text.x = element_text(colour = "#FFFFFF"))

### Departure Delays out of Austin
aus_dep <- austin_flights_df %>% filter(Origin == 'AUS') %>% 
  group_by(Dest,Origin) %>% summarize(delay_times = 
                                 mean(DepDelay[DepDelay>0],na.rm=T)) %>% ungroup() %>% 
  mutate(Dest = ifelse(Dest == "TUL","Tulsa",Dest),
    origin_loc = purrr::map(Origin, geocode, output = "latlon", source = "google", 
                                 messaging = FALSE),
         dest_loc = purrr::map(Dest, geocode, output = "latlon", source = "google", 
                               messaging = FALSE)) %>% unnest() %>% 
  rename("origin_lon"="lon","origin_lat"="lat","dest_lon"="lon1","dest_lat"="lat1")

## 75% quantile is 37.5
library(ggrepel)
aus_dep_text <- aus_dep %>% filter(delay_times > 37.5)

p2 <- ggplot(states) + 
  geom_polygon(aes(x = long, y = lat,group=group),color='black',fill='grey') + map.opts  + 
  coord_fixed(1.3) + 
  geom_point(data=aus_dep,aes(x=dest_lon,y=dest_lat,size=delay_times,color=delay_times)) +
  geom_text_repel(data=aus_dep_text,aes(x=dest_lon,y=dest_lat,label=Dest),
                  fontface = 'bold', color = 'blue',
                  box.padding = unit(0.35, "lines"),
                  point.padding = unit(0.5, "lines"),
                  segment.color = 'grey50') +  
  scale_color_gradient(low='yellow', high="red",name='Delay (minutes)') +
  guides(color=FALSE,size=FALSE) + 
  labs(title = "Average Departure Delay Times")


### Departure Delays heading to Austin. 
aus_arrival <- austin_flights_df %>% filter(Dest == 'AUS') %>% 
  group_by(Origin,Dest) %>% summarize(delay_times = 
                                        mean(ArrDelay[ArrDelay>0],na.rm=T)) %>% ungroup() %>% 
  mutate(Origin = ifelse(Origin == "TUL","Tulsa",Origin),
         origin_loc = purrr::map(Origin, geocode, output = "latlon", source = "google", 
                                 messaging = FALSE),
         dest_loc = purrr::map(Dest, geocode, output = "latlon", source = "google", 
                               messaging = FALSE)) %>% unnest() %>% 
  rename("origin_lon"="lon","origin_lat"="lat","dest_lon"="lon1","dest_lat"="lat1")

### Quantile 36.02
aus_arr_text <- aus_arrival %>% filter(delay_times > 36.02)

p3 <- ggplot(states) + 
  geom_polygon(aes(x = long, y = lat,group=group),color='black',fill='grey') + map.opts  + 
  coord_fixed(1.3) + 
  geom_point(data=aus_arrival,aes(x=origin_lon,y=origin_lat,size=delay_times,color=delay_times)) +
  geom_text_repel(data=aus_arr_text,aes(x=origin_lon,y=origin_lat,label=Origin),
                  fontface = 'bold', color = 'blue',
                  box.padding = unit(0.35, "lines"),
                  point.padding = unit(0.5, "lines"),
                  segment.color = 'grey50') +  
  scale_color_gradient(low='yellow', high="red",name='Delay (minutes)') +
  guides(color=FALSE,size=FALSE) + 
  labs(title = "Average Arrival Delay Times")

library(gridExtra)
library(grid)

grid.arrange(arrangeGrob(p2,p3,ncol=2),top = textGrob("Departure/Arrival Delays out of Austin-Bergstorm", gp = gpar(fontsize=18, fontface="bold.italic", fontsize=18)))

grid.arrange(p1,p4,ncol=2)
@

This code takes a significant amount of time to run due to geocoding requirements for each location. Currently, this chunk is set to eval=FALSE and .png files are included in place of direct code plots. If you desire to see direct code results, this can be changed and fresh plots will be created. \par


\includegraphics[width=\textwidth]{geo_plot}
\includegraphics[width=\textwidth]{cal_plot}

These plots show average delays on arrivals and departures. In just a few plots and with little external explanation, a reader can determine delays based upon calendar date and geographic location.

\end{homeworkProblem}

\pagebreak

\begin{homeworkProblem}

\textbf{Author Attribution}

Revisit the Reuters C50 corpus that we explored in class. Your task is to build two separate models (using any combination of tools you see fit) for predicting the author of an article on the basis of that article's textual content. Describe clearly what models you are using, how you constructed features, and so forth. (Yes, this is a supervised learning task, but it potentially draws on a lot of what you know about unsupervised learning!) \par

In the C50train directory, you have ~50 articles from each of 50 different authors (one author per directory). Use this training data (and this data alone) to build the two models. Then apply your model to the articles by the same authors in the C50test directory, which is about the same size as the training set. How well do your models do at predicting the author identities in this out-of-sample setting? Are there any sets of authors whose articles seem difficult to distinguish from one another? Which model do you prefer? \par

Note: you will need to figure out a way to deal with words in the test set that you never saw in the training set. This is a nontrivial aspect of the modeling exercise. \par

\textbf{Solution:}

To begin our pre-processing of data, we generated a readerPlain function to read in text files individually. We also used the glob function to identify the directory names for training and test sets. To perform the bulk of our processing, we then created a Process\_dir\_text function. This function appends the author names provided to a vector, after we provide function parameters such as directory names and weight types. From here, we used the vector of author names in order to read in each of the files, uniquely by author name. These documents (per author) were then compiled into a corpus. The corpus needed to be transformed – specifically removing numbers and punctuation, stripping whitespace, and removing stop words. With this clean corpus, we were able to weight the words within the Document Term Matrix appropriately.\par

After the Process\_dir\_text function was created, we applied the function to both the training and test data. To ensure that we extracted the correct author names to the target column for our predictions, we filtered our author names via a regex. Finally, with all pre-processing complete, we would be able to implement our prediction models. \par

For the first of our two models, we decided to use an unsupervised learning approach by applying the Naïve Bayes model. To determine which the best fit model, we initially weighted the terms based on the number of times they appeared in the data set (Term Frequency – TF). \par

<<echo=FALSE,message=FALSE,warning=FALSE>>=
setwd("~/STA380/data/ReutersC50")

readerPlain = function(fname){
  readPlain(elem=list(content=readLines(fname)), 
            id=fname, language='en') }

library(tm)
## Get List of directories 
author_train_dir = Sys.glob('C50train/*') # train
author_test_dir = Sys.glob('C50test/*') # test

### Create function to process files into a matrix. 
process_dir_text <- function(dir_names,type="tf"){
  file_list = NULL
  labels = NULL
  for(author in dir_names) {
    author_name = substring(author, first=9)
    files_to_add = Sys.glob(paste0(author, '/*.txt'))
    file_list = append(file_list, files_to_add)
    labels = append(labels, rep(author_name, length(files_to_add)))
  }
  all_docs = lapply(file_list, readerPlain) 
  names(all_docs) = file_list
  names(all_docs) = sub('.txt', '', names(all_docs))
  
  my_corpus = Corpus(VectorSource(all_docs))
  
  # Preprocessing
  my_corpus = tm_map(my_corpus, content_transformer(tolower)) # make everything lowercase
  my_corpus = tm_map(my_corpus, content_transformer(removeNumbers)) # remove numbers
  my_corpus = tm_map(my_corpus, content_transformer(removePunctuation)) # remove punctuation
  my_corpus = tm_map(my_corpus, content_transformer(stripWhitespace)) ## remove excess white-space
  my_corpus = tm_map(my_corpus, content_transformer(removeWords), stopwords("SMART"))
  
  if(type == "tf"){
    ## Term Frequency
    DTM = DocumentTermMatrix(my_corpus)
  }
  else{
    ## TF-IDF
    DTM = DocumentTermMatrix(my_corpus, control = list(weighting = weightTfIdf))
  }
  DTM = removeSparseTerms(DTM, 0.95)
  mat = as.matrix(DTM)
  ## Term frequency weighting
  if(type == "tf"){
    mat = mat/rowSums(mat)
  }
  rownames(mat) = file_list
  return(mat) # some basic summary statistics
}

mat_train <- process_dir_text(author_train_dir,"tf")
train_df <- as.data.frame(mat_train)
train_df$author <- gsub(".*[_]([^.]+)[_].*", "\\1", gsub("/","_",rownames(train_df)))
rownames(train_df) <- seq(1:nrow(train_df))

mat_test <- process_dir_text(author_test_dir,"tf")
test_df <- as.data.frame(mat_test)
test_df$author <- gsub(".*[_]([^.]+)[_].*", "\\1", gsub("/","_",rownames(test_df)))
rownames(test_df) <- seq(1:nrow(test_df))


### Run Models 
### Naive Bayes 
library(naivebayes)
#Create model, use laplace = 1 to smooth
model <- naive_bayes(author ~ ., data = train_df, laplace = 1)
#run model against withheld validation data
preds <- predict(model, newdata = test_df)
## confusion matrix
conf_mat <- table(preds,test_df$author)
accuracy <- sum(diag(conf_mat))/sum(conf_mat)
## print accuracies
print(paste("Naive Bayes accuracy: ", accuracy))
@

In doing this, we determined total accuracy to be approximately 47.4\% in predicting the testing data set with our model. This was low, but somewhat expect as the data provided as text is difficult to process and the training set was limited (only 50 samples per author). With this small of a sample size, and the fact that there is a large amount of intersection among word choice per author, this was an average prediction. In order to possibly improve this model, we also implemented term frequency inverse document matrix (TFIDF) to vary the weight of word importance. This, however, showed a regression to 40\% accuracy and therefore was not our strongest choice for Naïve Bayes. Ultimately, both models showed promise, but they left an opportunity for improved accuracy which led us to explore supervised learning methods. \par

<<echo=FALSE,warning=FALSE,results='asis'>>=
library(xtable)
acc_vec <- c()
for(i in 1:50){
  ind_value <- conf_mat[i,i]
  row_sum <- sum(conf_mat[i,])
  acc_vec[i] <- ind_value/row_sum
}
### calculate individaul accuracies per author
author_acc <- as.data.frame(cbind(rownames(conf_mat),acc_vec),stringsAsFactors = FALSE)
author_acc$acc_vec <- as.numeric(acc_vec)
colnames(author_acc) <- c("Authors","Accuracy")
author_acc_top <- author_acc[1:40,]
author_acc_bottom <- author_acc[41:50,]
@


<<echo=FALSE, results='asis'>>=
print(xtable(author_acc_top, caption = "Model accuracy by author"),
      include.rownames=FALSE,
      caption.placement = "top",
      table.placement='H')
@
These tables show the accuracy per author, which provided a bit more insight. The variation of accuracy shows how some authors can be found with a small sample size, but that some authors have similar enough writing styles that more articles would be required to properly and accurately differentiate between them. \par

<<echo=FALSE, results='asis'>>=
print(xtable(author_acc_bottom, caption = "Model accuracy by author"),
      caption.placement = "top",
      include.rownames=FALSE,
      table.placement='H')
@

Next for comparison, a model was made with random forests. \par

<<echo=FALSE, results='asis',message=FALSE>>=
### Different Model 
### Random Forest
train_filt <- train_df[,colnames(train_df) %in% colnames(test_df)]
train_filt$author = as.factor(train_filt$author)
train_author <- train_filt$author
train_filt$author <- NULL

test_filt <- test_df[,colnames(test_df) %in% colnames(train_df)]
actual <- as.factor(test_filt$author)
test_filt$author <- NULL

library(randomForest)

set.seed(1)
n = nrow(train_filt)
ntreev = c(100,500,1000)
nset = length(ntreev) #To build ntreev number of random forests
fmat = matrix(0,n,nset)
m = nrow(test_filt)

tree_acc_rf <- data.frame(ntreev,acc=0)

for(i in 1:nset) {
  rffit = randomForest(train_author~., data=train_filt, ntree=ntreev[i],maxnodes=20, na.action = na.exclude)
  prediction = predict(rffit, newdata = test_filt)
  conf_mat_rf <- table(prediction,actual)
  accuracy_rf <- sum(diag(conf_mat_rf))/sum(conf_mat_rf)
  tree_acc_rf$acc[i] = accuracy_rf
}

print(xtable(tree_acc_rf,caption = "Accuracy for each Random Forest"),
             include.rownames=FALSE,
             caption.placement = "top",
             table.placement="H")

@

Prior to running our supervised learning method, we had to clean the data once more. To do this, we decided to eliminate elements in our test data for which we did not have a corresponding training point. The tradeoff here was that all of these elements would not be correctly classified upon running our prediction model. However, since this was a small subset of our testing data (approximately 3.2\%), we decided as a team to move forward and found the tradeoff worth it. \par

Following the cleaning and elimination of some testing data elements, we implemented the Random Forest model. In the model, we tested three different forest sizes – 100, 500, and 1000 trees, respectively. Additionally, for each of these models, we settled for a default mtry parameter value and set our max nodes parameter to 20, so that the depth of each tree would not lead to overfitting. In running this model, we saw that the highest returned accuracy of 38.76\% was provided by the n=500 trees random forest model. Therefore, we did not see improvement by running a supervised learning approach. \par

To conclude, based on the information available, the Naïve-Bayes model using Term Frequency generated our strongest prediction. We have learned that text is extremely difficult to process and analyze, given limited data. In order to improve our results moving forward, we would need more text files to improve our prediction algorithms. \par



\end{homeworkProblem}

\pagebreak

\begin{homeworkProblem}

\textbf{Association Rule Mining}

Revisit the notes on association rule mining, and walk through the R example on music playlists: playlists.R and playlists.csv. Then use the data on grocery purchases in groceries.txt and find some interesting association rules for these shopping baskets. The data file is a list of baskets: one row per basket, with multiple items per row separated by commas -- you'll have to cobble together a few utilities for processing this into the format expected by the "arules" package. Pick your own thresholds for lift and confidence; just be clear what these thresholds are and how you picked them. Do your discovered item sets make sense? Present your discoveries in an interesting and concise way.\par

\textbf{Solution:}

<<message=FALSE,warning=FALSE,echo=FALSE>>=
library(arules)
library(tidyverse)
library(xtable)
trans_grocery <- read.transactions("https://raw.githubusercontent.com/jgscott/STA380/master/data/groceries.txt", 
                                   format = c("basket"), sep = ",", 
                                   rm.duplicates = FALSE)
@

Support = 1\% \par
With a support value of 0.01, we would be able to capture as many baskets as possible with as many grocery items. \par
Confidence = .35 \par
With a confidence value of 0.35, we felt this value was not too small to where it captured baskets that were not truly correlated, and it also was not too large of a confidence value to limit us to too narrow of a scope. \par
MaxLength  = 3 \par
Baskets would have three items or less. Most recipes or dishes implement a core of three ingredients. So it would make sense to see some form of correlation in baskets of this size. \par

<<warning=FALSE>>=
rul_groc <- apriori(trans_grocery, 
                         parameter=list(support=.01, confidence=.35, maxlen=3))
rul_groc_df <- data.frame(
                        lhs = labels(lhs(rul_groc)),
                        rhs = labels(rhs(rul_groc)), 
                        rul_groc@quality)
@

<<echo=FALSE,results='asis'>>=
top_10_lift <- rul_groc_df %>% arrange(-lift) %>% top_n(10,wt=lift)
print(xtable(top_10_lift,caption = "Top 10 results sorted by lift"),
             include.rownames=FALSE,
             caption.placement = "top",
             table.placement="H")
@
Analyzing the top 10 baskets sorted by lift, we are able to see that the baskets that show the highest lift are easily correlated. For example, we would expect baskets with fruits and vegetables to have other types of vegetables. Additionally, baskets with curd and whole milk can be expected to contain yogurt as well, as they are all dairy products. Looking at the confidence for these baskets, we can also see that several of them have high confidence values of > 0.5. This would indicate that shoppers are more likely than not to purchase the additional item.\par

<<echo=FALSE,results='asis'>>=
worst_10_lift <- rul_groc_df %>% arrange(lift) %>% top_n(-10,wt=lift)
print(xtable(worst_10_lift,caption = "Bottom 10 results sorted by lift"),
             include.rownames=FALSE,
             caption.placement = "top",
             table.placement="H")
@
In comparison, when we analyze the bottom 10 baskets sorted by lift, we are able to see that these baskets still show correlation. For example when we look at the first row, we see that baskets with berries are slightly expected to also have whole milk - but in considering whether we would personally buy berries with milk, it would certainly be possible. It is also worth noting that each of the bottom 10 baskets sorted by lift have whole milk as the secondary item. This also makes sense since whole milk is a common staple in households and an item that many people buy on a weekly basis. \par
Milk seems logical for a low value of lift due to the high level of confidence that already exists for a customer to purchase milk before other items increase it's likelihood. Lift cannot be as high for products that already have high purchase confidence. Finally, we see that the confidence values associated with these baskets are somewhat lower than for the top 10 (no value is above 0.4). Overall, the confidence values are still high though - one would expect shoppers with the items in the "lhs" column in their baskets to have anywhere between a 35 and 39\% chance to buy whole milk. \par

<<echo=FALSE,results='asis'>>=
top_10_conf <- rul_groc_df %>% arrange(-confidence) %>% top_n(10,wt=confidence)
print(xtable(top_10_conf,caption = "Top 10 results sorted by confidence"),
             include.rownames=FALSE,
             caption.placement = "top",
             table.placement="H")
@
When observing the top 10 baskets based on confidence, the results are similar in what was observed previously. Specifically, top basket in this subset is the same as in the above. What can be seen importantly with this split is the fact that all confidence values are above 50\% meaning that majority of the time, shoppers with the items in the lhs column, will also purchase the item in the rhs column. Additionally, as previously stated, whole milk (a typical staple) tended to show up in a most of these top 10 baskets, which was to be expected. \par


<<echo=FALSE,results='asis'>>=
worst_10_conf <- rul_groc_df %>% arrange(confidence) %>% top_n(-10,wt=confidence)
print(xtable(worst_10_conf,caption = "Bottom 10 results sorted by confidence"),
             include.rownames=FALSE,
             caption.placement = "top",
             table.placement="H")
@
As stated above, the bottom 10 baskets based on confidence are very similar to the bottom 10 baskets based on lift. Even while this table only shows the lowest confidence, because of what our confidence level was set at, all these are significant finds with at least 35\% confidence that the rhs item is in the basket with the lhs items. Lastly, what is again shown is that the rhs contains whole milk, a staple which many shoppers would already have in their baskets. \par


\end{homeworkProblem}

\end{document}
